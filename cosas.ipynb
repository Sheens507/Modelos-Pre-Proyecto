{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/qu4nt/extracci%C3%B3n-autom%C3%A1tica-de-frases-y-palabras-relevantes-en-espa%C3%B1ol-con-python-stanfordnlp-e0d3be5cb29d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize: Tokenizador. Separa un texto en oraciones, y las oraciones en palabras.<br>\n",
    "mwt: Tokenizador de palabras compuestas. Reconoce palabras como follow up o asistente virtual.<br>\n",
    "pos: Clasificador de clase de palabra. Asigna categorías tales como adjetivo o verbo a una palabra.<br>\n",
    "lemma: Lematizador. Toma una palabra y la transforma a su forma de diccionario. Por ejemplo, reconoce que la forma de diccionario de cantábamos es cantar.<br>\n",
    "depparse: Analizador de dependencias. Señala la función de una palabra e indica cuál es su relación con otras palabras de la misma oración. Puede indicar, por ejemplo, que en la oración amo el chocolate, la frase el chocolate es el objeto amado por la persona que habla.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocesado y modelado\n",
    "from sklearn import svm # clasificador support vector machines\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk # natural lenguaje tool kit para procesamiento de lenguaje natural\n",
    "\n",
    "# cambiador de formatos para factorizar un dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "\n",
    "\n",
    "# Configuración warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install stanza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade typing_extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "# stanza.download('es')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_tokenizar(texto):\n",
    "    # Eliminación de números\n",
    "    nuevo_texto = re.sub(\"\\d+\", ' ', texto)\n",
    "    # Se convierte todo el texto a minúsculas\n",
    "    nuevo_texto = nuevo_texto.lower()\n",
    "    # Eliminación de páginas web (palabras que empiezan por \"http\")\n",
    "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
    "    # Eliminación de signos de puntuación\n",
    "    regex = '[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\n",
    "    \\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~—]'\n",
    "    nuevo_texto = re.sub(regex , ' ', nuevo_texto)\n",
    "    # Eliminación de espacios en blanco múltiples\n",
    "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
    "    # Tokenización por palabras individuales\n",
    "    nuevo_texto = nuevo_texto.split(sep = ' ')\n",
    "    # Eliminación de tokens con una longitud < 2\n",
    "    nuevo_texto = [token for token in nuevo_texto if len(token) > 1]\n",
    "    # quitar los token que sean solamente de una letra repetidas 2 o mas veces\n",
    "    nuevo_texto = [token for token in nuevo_texto if not re.fullmatch(r'(.)\\1*', token)]\n",
    "    # une los tokens en un solo string\n",
    "    nuevo_texto = ' '.join(nuevo_texto)\n",
    "    \n",
    "    return(nuevo_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 13:23:23 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 5.66MB/s]                    \n",
      "2024-06-09 13:23:23 INFO: Downloaded file to C:\\Users\\Sheen\\stanza_resources\\resources.json\n",
      "2024-06-09 13:23:25 INFO: Loading these models for language: es (Spanish):\n",
      "==================================\n",
      "| Processor    | Package         |\n",
      "----------------------------------\n",
      "| tokenize     | ancora          |\n",
      "| mwt          | ancora          |\n",
      "| pos          | ancora_charlm   |\n",
      "| lemma        | ancora_nocharlm |\n",
      "| constituency | combined_charlm |\n",
      "| depparse     | ancora_charlm   |\n",
      "| sentiment    | tass2020_charlm |\n",
      "| ner          | conll02         |\n",
      "==================================\n",
      "\n",
      "2024-06-09 13:23:25 INFO: Using device: cpu\n",
      "2024-06-09 13:23:25 INFO: Loading: tokenize\n",
      "2024-06-09 13:23:25 INFO: Loading: mwt\n",
      "2024-06-09 13:23:25 INFO: Loading: pos\n",
      "2024-06-09 13:23:25 INFO: Loading: lemma\n",
      "2024-06-09 13:23:26 INFO: Loading: constituency\n",
      "2024-06-09 13:23:26 INFO: Loading: depparse\n",
      "2024-06-09 13:23:27 INFO: Loading: sentiment\n",
      "2024-06-09 13:23:27 INFO: Loading: ner\n",
      "2024-06-09 13:23:28 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra: deberían\tLema: deber\n",
      "Palabra: modernizar\tLema: modernizar\n",
      "Palabra: los\tLema: el\n",
      "Palabra: equipos\tLema: equipo\n",
      "Palabra: utilizados\tLema: utilizado\n",
      "Palabra: en\tLema: en\n",
      "Palabra: los\tLema: el\n",
      "Palabra: laboratorios\tLema: laboratorio\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline(lang='es')\n",
    "cadena_cats = \"deberían modernizar los equipos utilizados en los laboratorios.\"\n",
    "cadena_cats=limpiar_tokenizar(cadena_cats)\n",
    "doc = nlp(cadena_cats)\n",
    "\n",
    "# Imprimir las palabras y sus lemas\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(f'Palabra: {word.text}\\tLema: {word.lemma}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('deberían', 2, 'aux')\n",
      "('modernizar', 0, 'root')\n",
      "('los', 4, 'det')\n",
      "('equipos', 2, 'obj')\n",
      "('utilizados', 4, 'amod')\n",
      "('en', 8, 'case')\n",
      "('los', 8, 'det')\n",
      "('laboratorios', 5, 'obl')\n"
     ]
    }
   ],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(‘Cats’, dependo de la palabra 4, y es a quien responsabilizo de lo que voy a decir porque soy un Sujeto)<br>\n",
    "(‘fue’, dependo de la palabra 4, y soy un verbo copulativo — solo estoy aquí para unir cosas — )<br>\n",
    "(‘una’, dependo de la palabra 4, y soy un determinante — así que no me hagas mucho caso — )<br>\n",
    "(‘película’, soy la palabra más importante acá y no dependo de nadie — soy la 0 — , y soy el root o núcleo de esta oración)<br>\n",
    "(‘realmente’, dependo de la palabra 6 — de ‘terrible’ — y soy un adverbio modificador)<br>\n",
    "(‘terrible’, dependo de la palabra 4, y soy un adjetivo que modifica a esa palabra)<br>\n",
    "(‘.’, dependo de 4, y soy un punto — no le importo a nadie en realidad — )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"Sin embargo, no las tiene todas consigo Zombie y es, precisamente, cuando tiene que lograr mantener la tensión tras el impetuoso \n",
    "retorno cuando patina. El director pierde el pulso vibrante para dejarse llevar por una trama más apagada, sin chispa y del todo previsible. \n",
    "El segundo tercio del film se desploma. Ese montaje paralelo donde vemos, de un lado a Strode intentando reponerse y a Myers resucitando de sus \n",
    "cenizas y deambulando cual sombra por el campo en un dilatado regreso a Haddonfield, se desinfla. Previsible, aburrido, sin pulso. Y dejando \n",
    "entrever las costuras de un guión en el que su creador no logra sostener el ritmo. Las pesadillas de Strode y los asesinatos de Myers se vuelven \n",
    "aburridos. Y no solo por repetición insulsa sino también porque Zombie se desata con escenas más creativas, pero carentes de originalidad. Donde \n",
    "ese doble juego entre realidad y ensoñación se vuelve insistente, pierde sutileza para convertirse en una evidente falta de recursos e \n",
    "imaginación. Aquí debería haber dejado su impronta y solo consigue rozar lo convencional, incluso lo ridículo. Basta comprobar las apariciones \n",
    "del Dr. Loomis (Malcolm McDowell) que rozan lo cómico o la insistencia con la que nos quiere recordar ese sueño de Myers con angelical madre y \n",
    "el caballo blanco, que acaban tomando un protagonismo excesivo y perdiendo toda la sugerencia que debería acompañar para convertirse en una \n",
    "incómoda y repetida presencia que acaba entorpeciendo el avance de la historia en varias escenas. Lamentablemente el desagisado hace llegar al \n",
    "tramo final sin muchas esperanzas de sorpresa. Y menos aún de tensión. El largo regreso de Myers y el desenlace ya no recuperan la brillantez \n",
    "inicial. Y muy al contrario demuestra que quizás Zombie dispuso de una gran oportunidad para reivindicarse pero que no supo aprovechar.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retorno impetuoso\n",
      "pulso vibrante\n",
      "trama apagada\n",
      "tercio segundo\n",
      "montaje paralelo\n",
      "regreso dilatado\n",
      "repetición insulsa\n",
      "escenas creativas\n",
      "juego doble\n",
      "falta evidente\n",
      "convencional ridículo\n",
      "madre angelical\n",
      "caballo blanco\n",
      "protagonismo excesivo\n",
      "presencia incómoda\n",
      "tramo final\n",
      "regreso largo\n",
      "brillantez inicial\n",
      "oportunidad gran\n"
     ]
    }
   ],
   "source": [
    "doc_review = nlp(review)\n",
    "\n",
    "for sent in doc_review.sentences: # recorremos cada oración\n",
    "    for dep in sent.dependencies: # recorremos cada palabra\n",
    "        if dep[1] == 'amod': # el subíndice 1 indica que se trata de la clase funcional\n",
    "            print(dep[0].text, dep[2].text) # uso del atributo .text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Núcleo de 'satisfecho con todo.': satisfecho\n",
      "Núcleo de 'Modernidad en los equipos e instrumentos en los laboratorios y talleres: muchos de los instrumentos no están acorde con la lo que vende en el mercado actualmente.': Modernidad\n",
      "Núcleo de 'mejoramiento en la calidad de la internet y el acceso a mas amplios dentro de las instalaciones.': mejoramiento\n",
      "Núcleo de 'pocas charlas o seminarios de innovación de información enfocado para el grupo nocturno.': char\n",
      "Núcleo de 'Internet (WiFi Institucional). El internet debe de ser mejor debido a que pasamos más tiempo en la U que en otro lugar, y siempre estamos realizando investigaciones y tareas.': Internet\n"
     ]
    }
   ],
   "source": [
    "# import stanza\n",
    "\n",
    "# # Configurar stanza para el idioma español\n",
    "# stanza.download('es')\n",
    "# nlp = stanza.Pipeline('es')\n",
    "\n",
    "# Array de 5 oraciones de ejemplo\n",
    "oraciones = [\n",
    "    \"satisfecho con todo.\",\n",
    "    \"Modernidad en los equipos e instrumentos en los laboratorios y talleres: muchos de los instrumentos no están acorde con la lo que vende en el mercado actualmente.\",\n",
    "    \"mejoramiento en la calidad de la internet y el acceso a mas amplios dentro de las instalaciones.\",\n",
    "    \"pocas charlas o seminarios de innovación de información enfocado para el grupo nocturno.\",\n",
    "    \"Internet (WiFi Institucional). El internet debe de ser mejor debido a que pasamos más tiempo en la U que en otro lugar, y siempre estamos realizando investigaciones y tareas.\"\n",
    "]\n",
    "\n",
    "# Función para extraer la palabra núcleo de una oración\n",
    "def obtener_nucleo(oracion):\n",
    "    doc = nlp(oracion)\n",
    "    for sent in doc.sentences:\n",
    "        # Buscar la palabra núcleo (root) en la oración\n",
    "        for word in sent.words:\n",
    "            if word.head == 0:  # Si el head es 0, es el núcleo\n",
    "                return word.text\n",
    "\n",
    "# Iterar sobre las oraciones y mostrar el núcleo de cada una\n",
    "for oracion in oraciones:\n",
    "    nucleo = obtener_nucleo(oracion)\n",
    "    print(f\"Núcleo de '{oracion}': {nucleo}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Samsung_Campus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
